\begin{lstlisting}[language=Python, caption=Utilizzo di Transformers nel codice]
#Caricamento del Tokenizer e del Modello BERT
	from transformers import BertTokenizer, BertModel
	
	# Carica il tokenizer e il modello pre-addestrato di BERT
	tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
	bert_model = BertModel.from_pretrained('bert-base-uncased')
	
#Funzione per ottenere l'Embedding con BERT
	def get_bert_embedding(sentence):
	inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=50)
	with torch.no_grad():
	outputs = bert_model(**inputs)
	# Usa la media degli hidden states per rappresentare la frase
	return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
	
#Creazione dei Dati di Addestramento
	train_x = []
	train_y = []
	
	for pattern, tag in documents:
	# Ottieni l'embedding per la frase
	embedding = get_bert_embedding(pattern)
	train_x.append(embedding)
	
	# One-hot encoding del tag
	output_row = [0] * len(classes)
	output_row[classes.index(tag)] = 1
	train_y.append(output_row)
	
	train_x = np.array(train_x)
	train_y = np.array(train_y)
\end{lstlisting}